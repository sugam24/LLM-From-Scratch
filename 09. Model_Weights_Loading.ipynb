{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 Architecture & Weight Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        queries = self.W_query(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = self.W_value(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
        "        return self.out_proj(context_vec)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"], d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"], num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"], qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        return x + shortcut\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        return self.out_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Save/Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved and loaded!\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# Load model\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "model.eval()\n",
        "print(\"Model saved and loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load OpenAI Pretrained Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\"):\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    with open(os.path.join(model_dir, \"hparams.json\"), \"r\") as f:\n",
        "        settings = json.load(f)\n",
        "    ckpt_path = os.path.join(model_dir, \"model.ckpt\")\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "        variable_name_parts = name.split(\"/\")[1:]\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "            variable_name_parts = variable_name_parts[1:]\n",
        "        for key in variable_name_parts[:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "        target_dict[variable_name_parts[-1]] = variable_array\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"ln_f\"][\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"ln_f\"][\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained GPT-2 weights loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained weights\n",
        "settings, params = download_and_load_gpt2(\"124M\", \"gpt2\")\n",
        "gpt = GPTModel(GPT_CONFIG_124M)\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)\n",
        "gpt.eval()\n",
        "print(\"Pretrained GPT-2 weights loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "import os\n",
        "if os.path.exists(\"model.pth\"):\n",
        "    os.remove(\"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† GPT-2 Architecture & Weight Loading\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **GPT-2 Architecture Implementation** - Complete model from scratch\n",
        "2. **PyTorch Model Saving/Loading** - Save and restore trained models\n",
        "3. **Loading OpenAI Pretrained Weights** - Use official GPT-2 weights\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìê GPT-2 Model Configurations\n",
        "\n",
        "GPT-2 comes in 4 sizes. Here are the configurations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Available GPT-2 Configurations:\n",
            "------------------------------------------------------------\n",
            "Model                     Embedding    Layers     Heads   \n",
            "------------------------------------------------------------\n",
            "gpt2-small (124M)         768          12         12      \n",
            "gpt2-medium (355M)        1024         24         16      \n",
            "gpt2-large (774M)         1280         36         20      \n",
            "gpt2-xl (1558M)           1600         48         25      \n"
          ]
        }
      ],
      "source": [
        "# Base configuration for GPT-2 124M\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "# Model configurations for different GPT-2 sizes\n",
        "MODEL_CONFIGS = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "print(\"üìä Available GPT-2 Configurations:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Model':<25} {'Embedding':<12} {'Layers':<10} {'Heads':<8}\")\n",
        "print(\"-\" * 60)\n",
        "for name, config in MODEL_CONFIGS.items():\n",
        "    print(f\"{name:<25} {config['emb_dim']:<12} {config['n_layers']:<10} {config['n_heads']:<8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üèóÔ∏è GPT-2 Architecture Components\n",
        "\n",
        "The GPT-2 model consists of several key components:\n",
        "1. **MultiHeadAttention** - Self-attention mechanism\n",
        "2. **LayerNorm** - Layer normalization\n",
        "3. **GELU** - Activation function\n",
        "4. **FeedForward** - MLP block\n",
        "5. **TransformerBlock** - Combines attention and feedforward\n",
        "6. **GPTModel** - Complete model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1Ô∏è‚É£ Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MultiHeadAttention class defined\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)      # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "print(\"‚úÖ MultiHeadAttention class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2Ô∏è‚É£ Layer Normalization, GELU Activation & FeedForward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LayerNorm, GELU, and FeedForward classes defined\n"
          ]
        }
      ],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # Expansion\n",
        "            GELU(),                                          # Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),  # Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "print(\"‚úÖ LayerNorm, GELU, and FeedForward classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3Ô∏è‚É£ Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TransformerBlock class defined\n"
          ]
        }
      ],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"‚úÖ TransformerBlock class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4Ô∏è‚É£ Complete GPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPTModel class defined\n"
          ]
        }
      ],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ GPTModel class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ Test the Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Total parameters: 163,009,536\n",
            "üìä Parameters (with weight tying): 124,412,160\n",
            "üíæ Model size: 621.83 MB\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"üìä Total parameters: {total_params:,}\")\n",
        "\n",
        "# Accounting for weight tying (token embedding reused in output layer)\n",
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"üìä Parameters (with weight tying): {total_params_gpt2:,}\")\n",
        "\n",
        "# Memory size\n",
        "total_size_bytes = total_params * 4  # 4 bytes per float32\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"üíæ Model size: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üíæ Saving and Loading Model Weights in PyTorch\n",
        "\n",
        "The recommended way is to save a model's `state_dict`, a dictionary mapping each layer to its parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model weights saved to 'model.pth'\n"
          ]
        }
      ],
      "source": [
        "# Create a model instance\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# Save just the model weights\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"‚úÖ Model weights saved to 'model.pth'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model weights loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create a new model instance\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"‚úÖ Model weights loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving Model + Optimizer (for resuming training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model and optimizer saved to 'model_and_optimizer.pth'\n"
          ]
        }
      ],
      "source": [
        "# Create model and optimizer\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "# Save both model and optimizer state\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "}, \"model_and_optimizer.pth\")\n",
        "\n",
        "print(\"‚úÖ Model and optimizer saved to 'model_and_optimizer.pth'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Model + Optimizer (to resume training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model and optimizer restored for continued training!\n"
          ]
        }
      ],
      "source": [
        "# Load the checkpoint\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
        "\n",
        "# Create new instances\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "\n",
        "# Restore states\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "model.train()  # Set to training mode\n",
        "print(\"‚úÖ Model and optimizer restored for continued training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üåê Loading Pretrained Weights from OpenAI\n",
        "\n",
        "OpenAI openly shared the weights of their GPT-2 models. Let's load them into our architecture!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow>=2.15.0 tqdm>=4.66 requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download and Load GPT-2 Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ download_and_load_gpt2 function defined\n"
          ]
        }
      ],
      "source": [
        "def download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\"):\n",
        "    \"\"\"\n",
        "    Download GPT-2 model files from OpenAI and load the weights.\n",
        "    \n",
        "    Args:\n",
        "        model_size: One of \"124M\", \"355M\", \"774M\", or \"1558M\"\n",
        "        models_dir: Directory to save the model files\n",
        "    \n",
        "    Returns:\n",
        "        settings: Model configuration dictionary\n",
        "        params: Model weights dictionary\n",
        "    \"\"\"\n",
        "    # Create model directory\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    \n",
        "    # Download files if not present\n",
        "    if not os.path.exists(model_dir):\n",
        "        print(f\"üì• Downloading GPT-2 {model_size} model...\")\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        \n",
        "        base_url = f\"https://openaipublic.blob.core.windows.net/gpt-2/models/{model_size}\"\n",
        "        filenames = [\n",
        "            \"checkpoint\",\n",
        "            \"encoder.json\",\n",
        "            \"hparams.json\",\n",
        "            \"model.ckpt.data-00000-of-00001\",\n",
        "            \"model.ckpt.index\",\n",
        "            \"model.ckpt.meta\",\n",
        "            \"vocab.bpe\"\n",
        "        ]\n",
        "        \n",
        "        for filename in filenames:\n",
        "            url = f\"{base_url}/{filename}\"\n",
        "            filepath = os.path.join(model_dir, filename)\n",
        "            \n",
        "            response = requests.get(url, stream=True)\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            \n",
        "            with open(filepath, 'wb') as f:\n",
        "                with tqdm(total=total_size, unit='B', unit_scale=True, desc=filename) as pbar:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "        \n",
        "        print(\"‚úÖ Download complete!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ GPT-2 {model_size} model already downloaded.\")\n",
        "    \n",
        "    # Load hyperparameters\n",
        "    hparams_path = os.path.join(model_dir, \"hparams.json\")\n",
        "    with open(hparams_path, \"r\") as f:\n",
        "        settings = json.load(f)\n",
        "    \n",
        "    # Load weights from TensorFlow checkpoint\n",
        "    print(\"üîÑ Loading weights from TensorFlow checkpoint...\")\n",
        "    ckpt_path = os.path.join(model_dir, \"model.ckpt\")\n",
        "    \n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "    \n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip 'model' prefix\n",
        "        \n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "            variable_name_parts = variable_name_parts[1:]\n",
        "        \n",
        "        for key in variable_name_parts[:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "        \n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "    \n",
        "    print(\"‚úÖ Weights loaded successfully!\")\n",
        "    return settings, params\n",
        "\n",
        "print(\"‚úÖ download_and_load_gpt2 function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download GPT-2 124M Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPT-2 124M model already downloaded.\n",
            "üîÑ Loading weights from TensorFlow checkpoint...\n",
            "‚úÖ Weights loaded successfully!\n",
            "\n",
            "üìã Model Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "\n",
            "üîë Parameter dictionary keys: dict_keys(['blocks', 'ln_f', 'wpe', 'wte'])\n"
          ]
        }
      ],
      "source": [
        "# Download and load GPT-2 weights\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
        "\n",
        "print(\"\\nüìã Model Settings:\", settings)\n",
        "print(\"\\nüîë Parameter dictionary keys:\", params.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token embedding shape: (50257, 768)\n",
            "Position embedding shape: (1024, 768)\n"
          ]
        }
      ],
      "source": [
        "# Inspect token embedding weights\n",
        "print(\"Token embedding shape:\", params[\"wte\"].shape)\n",
        "print(\"Position embedding shape:\", params[\"wpe\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Model for OpenAI Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Model Configuration:\n",
            "  vocab_size: 50257\n",
            "  context_length: 1024\n",
            "  emb_dim: 768\n",
            "  n_heads: 12\n",
            "  n_layers: 12\n",
            "  drop_rate: 0.1\n",
            "  qkv_bias: True\n"
          ]
        }
      ],
      "source": [
        "# Select model configuration\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(MODEL_CONFIGS[model_name])\n",
        "\n",
        "# OpenAI used 1024 context length and bias in QKV projections\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "print(\"üìã Model Configuration:\")\n",
        "for key, value in NEW_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weight Assignment Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ assign function defined\n"
          ]
        }
      ],
      "source": [
        "def assign(left, right):\n",
        "    \"\"\"\n",
        "    Assign weights from right to left, checking shape compatibility.\n",
        "    \"\"\"\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "print(\"‚úÖ assign function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Weights into GPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ load_weights_into_gpt function defined\n"
          ]
        }
      ],
      "source": [
        "def load_weights_into_gpt(gpt, params):\n",
        "    \"\"\"\n",
        "    Load OpenAI GPT-2 weights into our GPTModel.\n",
        "    \n",
        "    Args:\n",
        "        gpt: GPTModel instance\n",
        "        params: Dictionary of weights from OpenAI checkpoint\n",
        "    \"\"\"\n",
        "    # Load embeddings\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    # Load transformer blocks\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        # Attention weights (Q, K, V are concatenated in OpenAI checkpoint)\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        # Attention biases\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        # Output projection\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # MLP weights\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # Layer norms\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    # Final layer norm\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"ln_f\"][\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"ln_f\"][\"b\"])\n",
        "    \n",
        "    # Output head (weight tied with token embeddings)\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "print(\"‚úÖ load_weights_into_gpt function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Model and Load Pretrained Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Pretrained GPT-2 weights loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create model with the correct configuration\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()\n",
        "\n",
        "# Load OpenAI weights\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)\n",
        "\n",
        "print(\"\\n‚úÖ Pretrained GPT-2 weights loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Text Generation with Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenizer and helper functions ready\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    \"\"\"Convert text to token IDs.\"\"\"\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    \"\"\"Convert token IDs back to text.\"\"\"\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "print(\"‚úÖ Tokenizer and helper functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Function with Temperature and Top-k Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ generate function defined\n"
          ]
        }
      ],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    \"\"\"\n",
        "    Generate text tokens from the model.\n",
        "    \n",
        "    Args:\n",
        "        model: GPTModel instance\n",
        "        idx: Input token indices (batch_size, seq_len)\n",
        "        max_new_tokens: Maximum number of new tokens to generate\n",
        "        context_size: Maximum context length\n",
        "        temperature: Sampling temperature (0 = greedy, higher = more random)\n",
        "        top_k: Only sample from top k tokens\n",
        "        eos_id: End of sequence token ID\n",
        "    \n",
        "    Returns:\n",
        "        Generated token indices\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "print(\"‚úÖ generate function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Generated text:\n",
            "--------------------------------------------------\n",
            "Every effort moves you toward finding an ideal new way to practice something!\n",
            "\n",
            "What makes us want to be on top of that?\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate text with the pretrained model\n",
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"üéØ Generated text:\")\n",
        "print(\"-\" * 50)\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Prompt: 'Artificial intelligence is'\n",
            "--------------------------------------------------\n",
            "Artificial intelligence is more than just a tool. It has more potential than science to uncover problems and discover new possibilities for humanity. Today's technologies give us insights into life's most profound and immediate dangers. They can provide insight into our world as we know it, and\n"
          ]
        }
      ],
      "source": [
        "# Try another prompt\n",
        "torch.manual_seed(42)\n",
        "\n",
        "prompt = \"Artificial intelligence is\"\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(prompt, tokenizer).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=40,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(f\"üéØ Prompt: '{prompt}'\")\n",
        "print(\"-\" * 50)\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ‚úÖ Summary\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "1. **GPT-2 Architecture** - Built the complete model from scratch:\n",
        "   - `MultiHeadAttention` - Self-attention with causal masking\n",
        "   - `LayerNorm`, `GELU`, `FeedForward` - Supporting components\n",
        "   - `TransformerBlock` - Combines attention and feedforward with residual connections\n",
        "   - `GPTModel` - Complete architecture with embeddings and output head\n",
        "\n",
        "2. **PyTorch Model Saving/Loading**:\n",
        "   - `torch.save(model.state_dict(), \"model.pth\")` - Save weights\n",
        "   - `model.load_state_dict(torch.load(\"model.pth\"))` - Load weights\n",
        "   - Save/load optimizer state for resuming training\n",
        "\n",
        "3. **OpenAI Pretrained Weights**:\n",
        "   - Download from OpenAI's public blob storage\n",
        "   - Convert TensorFlow checkpoint to PyTorch\n",
        "   - Generate coherent text with the pretrained model\n",
        "\n",
        "The model is now ready for fine-tuning on specific tasks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Saved model files:\n",
            "--------------------------------------------------\n",
            "‚úÖ model.pth (669.88 MB)\n",
            "   Path: /content/model.pth\n",
            "‚úÖ model_and_optimizer.pth (669.89 MB)\n",
            "   Path: /content/model_and_optimizer.pth\n",
            "--------------------------------------------------\n",
            "\n",
            "üí° You can load these files with:\n",
            "   torch.load('model.pth', weights_only=True)\n",
            "   torch.load('model_and_optimizer.pth', weights_only=True)\n"
          ]
        }
      ],
      "source": [
        "# Check saved model files\n",
        "import os\n",
        "\n",
        "saved_files = [\"model.pth\", \"model_and_optimizer.pth\"]\n",
        "print(\"üìÅ Saved model files:\")\n",
        "print(\"-\" * 50)\n",
        "for f in saved_files:\n",
        "    if os.path.exists(f):\n",
        "        size_mb = os.path.getsize(f) / (1024 * 1024)\n",
        "        full_path = os.path.abspath(f)\n",
        "        print(f\"‚úÖ {f} ({size_mb:.2f} MB)\")\n",
        "        print(f\"   Path: {full_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {f} - not found\")\n",
        "print(\"-\" * 50)\n",
        "print(\"\\nüí° You can load these files with:\")\n",
        "print(\"   torch.load('model.pth', weights_only=True)\")\n",
        "print(\"   torch.load('model_and_optimizer.pth', weights_only=True)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
