{"cells":[{"cell_type":"markdown","metadata":{"id":"WraJutopnTSl"},"source":["### GPT Model class we coded earlier"]},{"cell_type":"code","source":["# This block contains the full code of the architecture of GPT-2 model, which we constructed in file 04. LLM_architecture(GPT).ipynb\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","# -------------------------\n","# Configuration\n","# -------------------------\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,\n","    \"context_length\": 1024,\n","    \"emb_dim\": 768,\n","    \"n_heads\": 12,\n","    \"n_layers\": 12,\n","    \"drop_rate\": 0.1,\n","    \"qkv_bias\": False\n","}\n","\n","\n","# -------------------------\n","# Layer Normalization\n","# -------------------------\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim, eps=1e-5):\n","        super().__init__()\n","        self.eps = eps\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n","\n","\n","# -------------------------\n","# GELU Activation\n","# -------------------------\n","class GELU(nn.Module):\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) *\n","            (x + 0.044715 * x**3)\n","        ))\n","\n","\n","# -------------------------\n","# Feed Forward Network\n","# -------------------------\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","            nn.Dropout(cfg[\"drop_rate\"])\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","# -------------------------\n","# Multi-Head Self-Attention\n","# -------------------------\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        assert cfg[\"emb_dim\"] % cfg[\"n_heads\"] == 0\n","\n","        self.n_heads = cfg[\"n_heads\"]\n","        self.head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n","\n","        self.qkv = nn.Linear(\n","            cfg[\"emb_dim\"],\n","            3 * cfg[\"emb_dim\"],\n","            bias=cfg[\"qkv_bias\"]\n","        )\n","        self.proj = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n","        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(\n","                torch.ones(cfg[\"context_length\"], cfg[\"context_length\"]),\n","                diagonal=1\n","            )\n","        )\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","\n","        qkv = self.qkv(x)\n","        q, k, v = qkv.chunk(3, dim=-1)\n","\n","        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n","        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n","        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n","\n","        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n","        att = att.masked_fill(self.mask[:T, :T].bool(), float(\"-inf\"))\n","        att = self.dropout(torch.softmax(att, dim=-1))\n","\n","        out = att @ v\n","        out = out.transpose(1, 2).contiguous().view(B, T, C)\n","        return self.proj(out)\n","\n","\n","# -------------------------\n","# Transformer Block\n","# -------------------------\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.ln1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.attn = MultiHeadAttention(cfg)\n","        self.ln2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.ff = FeedForward(cfg)\n","        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        x = x + self.dropout(self.attn(self.ln1(x)))\n","        x = x + self.dropout(self.ff(self.ln2(x)))\n","        return x\n","\n","\n","# -------------------------\n","# GPT Model\n","# -------------------------\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n","        )\n","\n","        self.ln_f = LayerNorm(cfg[\"emb_dim\"])\n","        self.head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n","\n","    def forward(self, idx):\n","        B, T = idx.shape\n","        tok = self.tok_emb(idx)\n","        pos = self.pos_emb(torch.arange(T, device=idx.device))\n","        x = self.drop_emb(tok + pos)\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        return self.head(x)\n"],"metadata":{"id":"gOoVaXYSsD12","executionInfo":{"status":"ok","timestamp":1769420973374,"user_tz":-345,"elapsed":2,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqpqP6sznTSl"},"source":["### Using GPT to generate text"]},{"cell_type":"markdown","metadata":{"id":"G2SEoN80nTSl"},"source":["<div class=\"alert alert-block alert-success\">\n","We initialize a GPT model using the code from the previous chapter\n","</div>"]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x"],"metadata":{"id":"RGt2apk8pXNn","executionInfo":{"status":"ok","timestamp":1769420973738,"user_tz":-345,"elapsed":10,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"qvCMIB_WpWDn","executionInfo":{"status":"ok","timestamp":1769420985651,"user_tz":-345,"elapsed":8793,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","execution_count":70,"metadata":{"id":"9AQyIgHtnTSl","executionInfo":{"status":"ok","timestamp":1769420985669,"user_tz":-345,"elapsed":2,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[],"source":["import torch\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference"]},{"cell_type":"markdown","metadata":{"id":"laUYkvz1nTSm"},"source":["<div class=\"alert alert-block alert-warning\">\n","    \n","We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n","\n","This is so that more readers will be able to follow and execute the code examples on their laptop computer\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"M3hjB_wunTSm"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","Next, we use the generate_text_simple function from the previous chapter to generate text.\n","\n","In addition, we define two convenience functions, text_to_token_ids and token_ids_to_text, for converting between token and text representations that we use throughout this chapter\n","</div>"]},{"cell_type":"code","source":["import torch\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    for _ in range(max_new_tokens):\n","\n","        # Crop context if it exceeds model limit\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Forward pass\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Use last token logits\n","        logits = logits[:, -1, :]\n","\n","        # Convert to probabilities\n","        probs = torch.softmax(logits, dim=-1)\n","\n","        # Greedy decoding\n","        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n","\n","        # Append token\n","        idx = torch.cat((idx, idx_next), dim=1)\n","\n","    return idx\n"],"metadata":{"id":"GGBlW_azseAR","executionInfo":{"status":"ok","timestamp":1769420985687,"user_tz":-345,"elapsed":17,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","execution_count":72,"metadata":{"id":"ukJpmMGsnTSm","outputId":"8b676568-badb-4f08-a074-f060158f43f5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985702,"user_tz":-345,"elapsed":14,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"]}],"source":["import tiktoken\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort moves you\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"jGYbaXq9nTSm"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","As we can see above, the model does not produce good text because it has not been trained yet\n","\n","How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n","\n","The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n","\n","The next chapters on finetuning LLMs will also introduce additional ways to measure model quality\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"uOaj38ktnTSm"},"source":["### Calculating the text generation loss: cross-entropy and perplexity"]},{"cell_type":"markdown","metadata":{"id":"xI95LFtUnTSm"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","Suppose we have an inputs tensor containing the token IDs for 2 training examples (rows)\n","\n","Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate\n","\n","Notice that the targets are the inputs shifted by 1 position, as explained in chapter 2 when we implemented the data loader\n","\n","</div>"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"0BAjpiALnTSm","executionInfo":{"status":"ok","timestamp":1769420985737,"user_tz":-345,"elapsed":25,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[],"source":["inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n","                       [40,    1107, 588]])   #  \"I really like\"]\n","\n","targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n","                        [1107,  588, 11311]]) #  \" really like chocolate\"]"]},{"cell_type":"markdown","metadata":{"id":"76m-fpJ_nTSm"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each\n","\n","Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n","\n","Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores\n","\n","</div>"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"W_z79cu2nTSm","outputId":"2d23f9e5-178c-4ee4-e2a7-796820788642","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985753,"user_tz":-345,"elapsed":4,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 50257])\n"]}],"source":["with torch.no_grad():\n","    logits = model(inputs)\n","\n","probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n","print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"acRPE97LnTSm"},"source":["<div class=\"alert alert-block alert-warning\">\n","    \n","As discussed in the previous chapter, we can apply the argmax function to convert the probability scores into predicted token IDs.\n","\n","The softmax function above produced a 50,257-dimensional vector for each token; the argmax function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token.\n","\n","Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:                                                                                                                                                                                         \n","</div>"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"l1Xr3XsgnTSm","outputId":"06b357a0-9d29-4d0a-a755-1d1db6f95a33","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985758,"user_tz":-345,"elapsed":4,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[[16657],\n","         [  339],\n","         [42826]],\n","\n","        [[49906],\n","         [29669],\n","         [41751]]])\n"]}],"source":["token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n","print(\"Token IDs:\\n\", token_ids)"]},{"cell_type":"markdown","metadata":{"id":"jVkicvUCnTSn"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens.\n","\n","That's because the model wasn't trained yet.\n","\n","To train the model, we need to know how far it is away from the correct predictions (targets)\n","\n","</div>"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"7opPX1MRnTSn","outputId":"58387ecc-f256-4065-bf2e-4e791c9afff5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985768,"user_tz":-345,"elapsed":9,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Targets batch 1:  effort moves you\n","Outputs batch 1:  Armed heNetflix\n"]}],"source":["print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n","print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"]},{"cell_type":"markdown","metadata":{"id":"mQLNYDgknTSn"},"source":["### Cross-entropy loss"]},{"cell_type":"markdown","metadata":{"id":"s7JxqDi3nTSn"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","The token probabilities corresponding to the target indices are as follows:\n","\n","\n","</div>"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"sYcECPqknTSn","outputId":"14f4a853-653e-4f9f-ac8e-cf1530b1aaf4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985769,"user_tz":-345,"elapsed":8,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n","Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"]}],"source":["text_idx = 0\n","target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 1:\", target_probas_1)\n","\n","text_idx = 1\n","target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 2:\", target_probas_2)"]},{"cell_type":"markdown","metadata":{"id":"DBcTJXvjnTSn"},"source":["<div class=\"alert alert-block alert-warning\">\n","\n","We want to maximize all these values, bringing them close to a probability of 1.\n","    \n","In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself.\n","\n","</div>"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"z9bRnHdCnTSn","outputId":"0c92f191-23c1-4da1-fcc8-e98e0c027d99","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985769,"user_tz":-345,"elapsed":4,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"]}],"source":["# Compute logarithm of all token probabilities\n","log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n","print(log_probas)"]},{"cell_type":"markdown","metadata":{"id":"GK2kkIVtnTSn"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","Next, we compute the average log probability:\n","</div>"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"pmBFQ3eLnTSo","outputId":"0243f66b-64f1-4336-b132-8353ff3ead06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985776,"user_tz":-345,"elapsed":7,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-10.7940)\n"]}],"source":["# Calculate the average probability for each token\n","avg_log_probas = torch.mean(log_probas)\n","print(avg_log_probas)"]},{"cell_type":"markdown","metadata":{"id":"DC54hfmrnTSo"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","The goal is to make this average log probability as large as possible by optimizing the model weights.\n","\n","Due to the log, the largest possible value is 0, and we are currently far away from 0.\n","\n","In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0.\n","\n","The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning.\n","\n","</div>"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"KYnUZHFMnTSo","outputId":"b4275369-f5cb-4d11-8154-d58271a11b6d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985776,"user_tz":-345,"elapsed":4,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.7940)\n"]}],"source":["neg_avg_log_probas = avg_log_probas * -1\n","print(neg_avg_log_probas)"]},{"cell_type":"markdown","metadata":{"id":"bbsjmFtOnTSo"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","PyTorch already implements a cross_entropy function that carries out the previous steps\n","\n","Before we apply the cross_entropy function, let's check the shape of the logits and targets\n","\n","</div>"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"PcBz-B2inTSo","outputId":"66a23d4e-07c3-43db-e71a-824a06064c1b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985781,"user_tz":-345,"elapsed":5,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Logits shape: torch.Size([2, 3, 50257])\n","Targets shape: torch.Size([2, 3])\n"]}],"source":["# Logits have shape (batch_size, num_tokens, vocab_size)\n","print(\"Logits shape:\", logits.shape)\n","\n","# Targets have shape (batch_size, num_tokens)\n","print(\"Targets shape:\", targets.shape)"]},{"cell_type":"markdown","metadata":{"id":"PWsGmwMxnTSp"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:\n","\n","\n","</div>"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"6K6CoSsTnTSp","outputId":"2746cc86-5725-4109-c97a-9d16ce893100","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420985785,"user_tz":-345,"elapsed":3,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Flattened logits: torch.Size([6, 50257])\n","Flattened targets: torch.Size([6])\n"]}],"source":["logits_flat = logits.flatten(0, 1)\n","targets_flat = targets.flatten()\n","\n","print(\"Flattened logits:\", logits_flat.shape)\n","print(\"Flattened targets:\", targets_flat.shape)"]},{"cell_type":"markdown","metadata":{"id":"oUG3gNlMnTSp"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize.\n","    \n","The cross_entropy function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized\n","\n","</div>"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"7Kgi5a61nTSp","outputId":"29ce86db-807f-41dc-a70a-f7fb449577e0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420995191,"user_tz":-345,"elapsed":9,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.7940)\n"]}],"source":["loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"Aof7UYpmnTSp"},"source":["### Perplexity"]},{"cell_type":"markdown","metadata":{"id":"d1uOGBo1nTSp"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","A concept related to the cross-entropy loss is the perplexity of an LLM.\n","\n","The perplexity is simply the exponential of the cross-entropy loss.\n","\n","</div>"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"knpahbDcnTSp","outputId":"dfec2573-5fb8-4c57-cd0a-0da6cdecdb58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420995213,"user_tz":-345,"elapsed":16,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(48725.8203)\n"]}],"source":["perplexity = torch.exp(loss)\n","print(perplexity)"]},{"cell_type":"markdown","metadata":{"id":"7NJe5fSjnTSp"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens).\n","\n","In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n","    \n","Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"pZNC0bhznTSp"},"source":["### Calculating the training and validation set losses"]},{"cell_type":"markdown","metadata":{"id":"Hu5ILepxnTSp"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","We use a relatively small dataset for training the LLM (in fact, only one short story)\n","\n","The reasons are:\n","\n","You can run the code examples in a few minutes on a laptop computer without a suitable GPU.\n","\n","The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes.\n","    \n","We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size.\n","    \n","For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n","\n","At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately 30 dollars.\n","\n","So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * 30 = 690,000 dollars\n","\n","Below, we use the same dataset we used in chapter 2.\n","\n","</div>"]},{"cell_type":"code","execution_count":85,"metadata":{"id":"Khnp56hAnTSq","executionInfo":{"status":"ok","timestamp":1769420995231,"user_tz":-345,"elapsed":12,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[],"source":["import os\n","import urllib.request\n","\n","file_path = \"the-verdict.txt\"\n","url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n","\n","if not os.path.exists(file_path):\n","    with urllib.request.urlopen(url) as response:\n","        text_data = response.read().decode('utf-8')\n","    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_data)\n","else:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        text_data = file.read()"]},{"cell_type":"markdown","metadata":{"id":"pz9czsB2nTSq"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","A quick check that the text loaded ok by printing the first and last 100 words\n","\n","</div>"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"JioLgRx9nTSq","outputId":"df04f735-7e3c-4b58-bbe4-4db176f3f24f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420995248,"user_tz":-345,"elapsed":9,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}],"source":["# First 100 characters\n","print(text_data[:99])"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"TVyJiBDNnTSq","outputId":"2a84f264-cb47-46ae-a2a9-5360c78094ea","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420995376,"user_tz":-345,"elapsed":4,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"]}],"source":["# Last 100 characters\n","print(text_data[-99:])"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"aAlKr7_xnTSq","outputId":"7e31e01d-494e-4789-965f-01bd4d50a889","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769420995412,"user_tz":-345,"elapsed":36,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Characters: 20479\n","Tokens: 5145\n"]}],"source":["total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)"]},{"cell_type":"markdown","metadata":{"id":"ysWEjuYLnTSq"},"source":["<div class=\"alert alert-block alert-warning\">\n","\n","With 5,145 tokens, the text is very short for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later).\n","\n","Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training.\n","    \n","For visualization purposes, the figure below assumes a max_length=6, but for the training loader, we set the max_length equal to the context length that the LLM supports.\n","\n","Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position    \n","</div>"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, text, tokenizer, max_length, stride):\n","        self.token_ids = tokenizer.encode(text)\n","        self.max_length = max_length\n","        self.stride = stride\n","\n","        self.inputs = []\n","        self.targets = []\n","\n","        for i in range(0, len(self.token_ids) - max_length, stride):\n","            x = self.token_ids[i : i + max_length]\n","            y = self.token_ids[i + 1 : i + max_length + 1]\n","            self.inputs.append(torch.tensor(x, dtype=torch.long))\n","            self.targets.append(torch.tensor(y, dtype=torch.long))\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs[idx], self.targets[idx]\n","\n","\n","def create_dataloader_v1(\n","    text,\n","    batch_size,\n","    max_length,\n","    stride,\n","    drop_last,\n","    shuffle,\n","    num_workers\n","):\n","    dataset = GPTDatasetV1(\n","        text=text,\n","        tokenizer=tokenizer,   # tokenizer must already exist\n","        max_length=max_length,\n","        stride=stride\n","    )\n","\n","    return DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n"],"metadata":{"id":"qkDkZ6mItOpY","executionInfo":{"status":"ok","timestamp":1769421068036,"user_tz":-345,"elapsed":62,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","execution_count":92,"metadata":{"id":"SV0aDRqbnTSr","executionInfo":{"status":"ok","timestamp":1769421070989,"user_tz":-345,"elapsed":29,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[],"source":["# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"56GU8ppUnTSr","executionInfo":{"status":"ok","timestamp":1769421074340,"user_tz":-345,"elapsed":20,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[],"source":["# Sanity check\n","\n","if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the training loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"increase the `training_ratio`\")\n","\n","if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the validation loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"decrease the `training_ratio`\")"]},{"cell_type":"markdown","metadata":{"id":"4VmV82vJnTSr"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with.\n","\n","Llama 2 7B was trained with a batch size of 1024, for example.\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"crbZRD6enTSr"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","An optional check that the data was loaded correctly:\n","\n","</div>"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"LBf8v30JnTSr","outputId":"ffcfbdaf-efb8-42bd-9a99-6be73306a21f","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1769421076066,"user_tz":-345,"elapsed":20,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","\n","Validation loader:\n","torch.Size([2, 256]) torch.Size([2, 256])\n"]}],"source":["print(\"Train loader:\")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"\\nValidation loader:\")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)"]},{"cell_type":"markdown","metadata":{"id":"fpDRuV6tnTSr"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","An optional check that the data was loaded correctly:\n","\n","</div>"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"Y5LY4ms3nTSs","outputId":"45618d6f-322a-46bb-a3b3-ec88ad4bc547","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769421080020,"user_tz":-345,"elapsed":14,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training tokens: 4608\n","Validation tokens: 512\n","All tokens: 5120\n"]}],"source":["train_tokens = 0\n","for input_batch, target_batch in train_loader:\n","    train_tokens += input_batch.numel()\n","\n","val_tokens = 0\n","for input_batch, target_batch in val_loader:\n","    val_tokens += input_batch.numel()\n","\n","print(\"Training tokens:\", train_tokens)\n","print(\"Validation tokens:\", val_tokens)\n","print(\"All tokens:\", train_tokens + val_tokens)"]},{"cell_type":"markdown","metadata":{"id":"wg6BeQ-ynTSs"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","Next, we implement a utility function to calculate the cross-entropy loss of a given batch.\n","\n","In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader.\n","</div>"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"kJ3hBoy0nTSw","executionInfo":{"status":"ok","timestamp":1769421085404,"user_tz":-345,"elapsed":87,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[],"source":["def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches"]},{"cell_type":"markdown","metadata":{"id":"TtRDXwFqnTSy"},"source":["<div class=\"alert alert-block alert-info\">\n","\n","If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n","    \n","Via the device setting, we ensure that the data is loaded onto the same device as the LLM model.\n","    \n","</div>"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"UdDGmHoPnTSz","outputId":"0ad7312c-c88a-4c73-fc57-815046b14195","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769421119389,"user_tz":-345,"elapsed":31432,"user":{"displayName":"Sugam Dahal","userId":"12886190372199512207"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 10.98758347829183\n","Validation loss: 10.98110580444336\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Note:\n","# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n","# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n","# However, the resulting loss values may be slightly different.\n","\n","#if torch.cuda.is_available():\n","#    device = torch.device(\"cuda\")\n","#elif torch.backends.mps.is_available():\n","#    device = torch.device(\"mps\")\n","#else:\n","#    device = torch.device(\"cpu\")\n","#\n","# print(f\"Using {device} device.\")\n","\n","\n","model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n","\n","\n","torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n","\n","with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device)\n","    val_loss = calc_loss_loader(val_loader, model, device)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[{"file_id":"1mH02fCTT0575ecaNnaDGAm2TTXvpXU3t","timestamp":1769419477096}]}},"nbformat":4,"nbformat_minor":0}